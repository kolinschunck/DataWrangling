{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Twitter - The Wrangle Report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Wrangle Report is a part of a Data Science Course Project offered by Udacity. The project aims to gather data from Twitter and combine it with a third party data frame to create analysis about the tweets and the predicted dog’s breed.\n",
    "\n",
    "\n",
    "## Data Gathering\n",
    "\n",
    "The project required gathering data from three different sources as listed below. For each of the data source a different method of data gathering was used namely:\n",
    "\n",
    "* Importing data via csv,\n",
    "* Using requests to download data, and\n",
    "* Scrape data from Twitter using an API\n",
    "\n",
    "Although the image_predictions.tsv file has almost all the information from the WeRateDogs™ user, there is some missed variable, which I have gathered using the tweepy package.\n",
    "\n",
    "\n",
    "### The Data Sources\n",
    "\n",
    "#### Twitter Archive Enhanced\n",
    "The WeRateDogs Twitter archive enhanced data file was provided by Udacity. The dataset contains basic tweet data for all 5000+ of their tweets, but not everything. The file was downloaded through the Udacity learning portal.\n",
    "\n",
    "#### Image Predictions\n",
    "The tweet image predictions (for example: what breed of dog (or other object, animal, etc.) is present in each tweet) is stored in a separate file. This file (image_predictions.tsv) is hosted on Udacity's servers and has been downloaded programmatically using the Requests library and the following URL: image_predictions.tsv.\n",
    "\n",
    "#### Data via Twitter API\n",
    "Then some missing variables were collected through an API of Twitter. This has been done by sign-up as a Twitter developer. Through the provided secure keys and tokens I was able to connect to the Twitter account of WeRateDogs™. I then followed the instructions given by Udacity, such as that I used the tweet IDs in the WeRateDogs™ Twitter archive to query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. Each tweet in the JSON data was written to its own line. Finally, I read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count.\n",
    "\n",
    "In general: the dataset were named in the wrangle_act file as follows:\n",
    "\n",
    "- df_twitter_archive: Loaded data frame from twitter_archive_enhanced.csv;\n",
    "- df_image_predictions: Loaded data frame from image_predictions.tsv, and;\n",
    "- df_twitter_archive_clean: Loaded data frame from twitter_archive_master.csv.\n",
    "\n",
    "\n",
    "## Assessing data\n",
    "\n",
    "Usually, data in this format does not come as we would like it. Hence, I had to identify isssues first. The data assessment process has found several issues, which I have detailed in below Table:\n",
    "\n",
    "\n",
    "|**Issue number**|**Description**|**Issue Type**| \n",
    "|---------------|---------------|--------------|\n",
    "|I#1| Combine three different dataframes into one master data set| Tidiness|\n",
    "|I#2| Clean sources columns| Quality| \n",
    "|I#3| Refine respective predictions and confidence columns| Quality|\n",
    "|I#4| Combine and clean different dog stages (eg: pupper,doggo) columns into one| Quality|\n",
    "|I#5| Clean text column to get dog gender| Quality|\n",
    "|I#6| Remove unwanted columns (retweeted_status_timestamp, retweeted_status_user_id, retweeted_status_id ) and clean up duplicate rows and NaNs| Tidiness|\n",
    "|I#7| Drop columns with one low values or similar kind of values|Quality|\n",
    "|I#8| Fix numerator and denominators| Quality|\n",
    "|I#9| Convert NaNs/Nulls to None | Quality|\n",
    "|I#10| Fix datatypes of various columns| Quality|\n",
    "\n",
    "\n",
    "## Cleaning Data\n",
    "\n",
    "The above issue list was then used as a guideline to clean the data. The process was enhanced using various sources from the internet, such as from stackoverflow and others (as listed in the resources section in wrangle_act.ipynb). \n",
    "For exmaple: There should be one master table instead of three tables. Thus, I merged the three dataframes into one. I have also merged 4 columns (doggo, pupper, puppo, and floofer) into one, which I have bundled and named as life_stage. Further, most of the issues involving non-usual values to rating_numerator and rating_denominator were solved using a new tailored regular expression to gather the ratings from text column. Also, it was important to convert NaN values to None type. The data type problems in, for example, tweet_id columns, were fixed using the .astype() method and .loc[]. Finally, I have solved the tidiness issues combining the tables twitter_archive_enhanced.csv and image_predictions.tsv in one called twitter_archive_master.csv.\n",
    "\n",
    "\n",
    "## Analyzing & Visualizing the Data\n",
    "\n",
    "The cleaned data is then used for analyzing and visualizing purposes. The results can be accessed through the act_report and wrangle_act. The files can be accessed through my GitHub profile.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "I have documented 10 issues. However, it should be noted that the file is most likely not free of all issues. Hence, a more in-depth assessment should be done in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
